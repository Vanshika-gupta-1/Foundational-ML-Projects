# -*- coding: utf-8 -*-
"""House pricing Dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17N6T6KWZd-smYOXaVewwDHmYVenaMIQM

**Importing libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

"""**Loading Dataset and Dataset details**"""

data = pd.read_csv("/content/1553768847-housing.csv")
print(data.head())

data.shape

data.info()

data.describe()

"""**Handling missing values**"""

data.isnull().sum()

"""**Checking unnecessary columns and drops**"""

data.nunique()

"""**Preprocessing**"""

# Check for missing values
print(data.isnull().sum())

# Drop rows with missing values (or use imputation if needed)
data.dropna(inplace=True)

# Optional: Rename target column
data.rename(columns={'median_house_value': 'target'}, inplace=True)

"""**Exploratory Data Analysis (EDA)**"""

print(data.duplicated().sum)

"""**correlation heatmap**"""

# Drop non-numeric columns for correlation heatmap
numeric_data = data.select_dtypes(include=['float64', 'int64'])

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

"""**Distribution of math score**"""

# Check column names to ensure the price column is correctly named
print(data.columns)

# Plot distribution of house prices using the correct column name
sns.histplot(data['target'], bins=30, kde=True)
plt.title("Distribution of House Prices")
plt.xlabel("Price")
plt.ylabel("Frequency")
plt.show()

"""**Bar Graph**"""

data = pd.read_csv("1553768847-housing.csv")

# Drop missing values if any
data.dropna(inplace=True)

# Select numerical columns you want to visualize
features_to_plot = ['housing_median_age', 'total_rooms', 'population', 'households']

# Calculate mean of selected features
feature_means = data[features_to_plot].mean()

# ✅ Now we can plot using feature_means
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_means.index, y=feature_means.values, palette='coolwarm')
plt.title("Average Value of Selected Housing Features")
plt.xlabel("Features")
plt.ylabel("Average Value")
plt.tight_layout()
plt.show()

"""**Pie Chart**"""

# Count the number of records in each category
category_counts = data['ocean_proximity'].value_counts()

# Plot the pie chart
plt.figure(figsize=(8, 8))
plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', startangle=140)
plt.title("Distribution of Houses by Ocean Proximity")
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle
plt.show()

"""**Prepare Data for Training**"""

# Drop non-numeric/categorical columns
data = data.drop(['ocean_proximity'], axis=1)

# Split features and target
X = data.drop('target', axis=1)
y = data['target']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Train Linear Regression Model**"""

model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

"""**Evaluate the Model**"""

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R² Score: {r2}")

"""**Visualization of Predictions**"""

plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted House Prices")
plt.show()

"""**XG Boost**"""

!pip install xgboost

model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print("R2 Score:", r2_score(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))

"""**Hyperparameter Tuning (GridSearchCV)**"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb

params = {
    'n_estimators': [50, 100],
    'max_depth': [3, 6],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1.0]
}

grid_search = GridSearchCV(
    estimator=xgb.XGBRegressor(objective='reg:squarederror'),
    param_grid=params,
    scoring='neg_root_mean_squared_error',
    cv=3,
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

# Predict using best model
y_pred_best = best_model.predict(X_test)
print("Best R2 Score:", r2_score(y_test, y_pred_best))
print("Best RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_best)))

"""**Train XGBoost Regressor (Basic)**"""

model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print("R2 Score:", r2_score(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))

"""**Feature Importance (Optional Visualization)**"""

import matplotlib.pyplot as plt

xgb.plot_importance(best_model, height=0.5)
plt.title("Feature Importance")
plt.show()